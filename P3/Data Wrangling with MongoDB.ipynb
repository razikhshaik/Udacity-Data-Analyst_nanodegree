{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For the 3rd project in the Data Analyst Course from Udacity, I am tasked to assess the quality of the data and prepare it for any further analysis. The data i will be working on is provided by OpenStreetMap(Open source geographic data for the world), my area of interest was to explore the city of __Toronto__. OpenStreetMap has metro extract for Toronto readily available to download but this version of data compromises on the boundary of the city, there is a lot more data correspodning to the surroundings of the city which should not have been there. For this reason, I have custom selected and downloaded the data  for the area that most likely resembles the complete city.\n",
    "\n",
    "Here's link to the data that i will be working on [Toronto data](http://overpass-api.de/api/map?bbox=-79.5795,43.5943,-79.1537,43.8365)\n",
    "<br>\n",
    "\n",
    "Toronto has always been a favorite city of mine for various reasons, of which the prominent one's being the time!!! The time I have spent there with my family and friends is unparalleled. I was exploring the city's top notch one of a kind architecture, restaurants, bars, galleries, buildings and parks. City boosts a very vibrant vibe, there is something for everyone to really enjoy here. Toronto is Canada's most populous city known for its diverse culture and is considered as very strong econimical hub.\n",
    "\n",
    "Let's get started!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with, lets first import all the necessary libraries. The data is in XML format, to parse the data we would require xml package. \n",
    "\n",
    "Often times the data that we are dealing with may be quite huge which makes it not possible to load the entire data in to the memory. Making use of iterative parsing can help overcome this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing all the required libraries\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "\n",
    "\n",
    "# the xml file that i downloaded from OpenStreetMap\n",
    "source_xml_file = \"toronto_canada.osm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit Tags\n",
    "\n",
    "XML contains various tag elements to distinguish one from another. Let's figure out which are the most used tags in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag - 1580816\n",
      "nd - 1487924\n",
      "node - 1291949\n",
      "way - 223417\n",
      "member - 69451\n",
      "relation - 2895\n",
      "bounds - 1\n",
      "note - 1\n",
      "meta - 1\n",
      "osm - 1\n"
     ]
    }
   ],
   "source": [
    "tag_dict =  count_tags(source_xml_file)\n",
    "print_dict_by_values(tag_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no unusual stuff going on with tags, all look to be perfectly alright as per the wiki page.\n",
    "\n",
    "\n",
    "\n",
    "## Audit Keys Type\n",
    "\n",
    "All the elements in the XML file have keys, these keys are associated with values which provide the information on that particular node.\n",
    "\n",
    "Let's audit for any problematic keys and what kind of various keys we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 938554,\n",
      " 'lower with colon': 617526,\n",
      " 'other': 24736,\n",
      " 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "keys_type, key_count = process_map(source_xml_file)\n",
    "\n",
    "pprint.pprint(keys_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, there are no problematic keys  that we have to deal with.\n",
    "\n",
    "Let's look at the unique keys and how many times they are repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source - 285114\n",
      "addr:street - 159311\n",
      "addr:housenumber - 158833\n",
      "addr:city - 140086\n",
      "highway - 112109\n",
      "addr:interpolation - 68713\n",
      "name - 65714\n",
      "surface - 53120\n",
      "lanes - 44810\n",
      "building - 44442\n"
     ]
    }
   ],
   "source": [
    "print_dict_by_values(key_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OSM allows the map to include an unlimited number of attributes describing each feature. The community agrees on certain key and value combinations for the most commonly used tags, which act as informal standards. \n",
    "\n",
    "Let's Audit how many of the keys present in the data are of informal standards as mentioned on the [wiki website](https://wiki.openstreetmap.org/wiki/Map_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shop', 'seasonal', 'maxspeed', 'office', 'healthcare:speciality']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_page = \"https://wiki.openstreetmap.org/wiki/Map_Features\"\n",
    "\n",
    "# Scraping the data from internet to validate how many keys that\n",
    "# are in the data are primary features! \n",
    "def extract_data(page):\n",
    "    ...\n",
    "    return primary_keys\n",
    "\n",
    "primary_keys_wiki = extract_data(html_page)\n",
    "list(primary_keys_wiki)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output displayed above are the primarily used key types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys that are not listed as Primary: 620\n",
      "Keys that are listed as Primary: 410\n"
     ]
    }
   ],
   "source": [
    "print \"Keys that are not listed as Primary: \" + str(len(not_in_primary_keys))\n",
    "print \"Keys that are listed as Primary: \" + str(len(in_primiary_keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It becomes a daunting task to validate the 620 keys as the OSM provides flexibility to user to create custom keys. There were many spelling mistakes and inappropriate data that is entered. If there is a formal list of standard keys, these keys can be validated against them to keep accurate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit Users\n",
    "\n",
    "In this step, I want to audit uid to see if they are actually all machine genereted intergers with no formatting errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function returns the user id from xml element\n",
    "def get_user(element):\n",
    "    return element.get(\"uid\")\n",
    "\n",
    "# This function checks for all the user id and if there\n",
    "# are any formatting issue with uid and any incorrect uid\n",
    "def process_users(filename):\n",
    "    ...\n",
    "    return [users, invalid_uid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1250 , There are 0 invalid uid's\n"
     ]
    }
   ],
   "source": [
    "users, invalid_users=process_users(source_xml_file)\n",
    "print \"Out of \" + str(len(users)) + \" , There are \"+\n",
    "      str(len(invalid_users)) + \" invalid uid's\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems Encountered in the Map\n",
    "\n",
    "## Audit/ Clean Street Type\n",
    "\n",
    "The first problem encountered in the map is the one about the street type. As we can see in the following code, words can become abbreviations or just a uppercase word instead of a lowercase word. There were spelling mistakes in the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge :  \n",
      "set(['Basking Ridge',\n",
      "     'Bayview Ridge',\n",
      "     'Circle Ridge',\n",
      "     'Dianawood Ridge',\n",
      "     'Echo Valley Ridge',\n",
      "     'Hunting Ridge',\n",
      "     'Orchard Haven Ridge'])\n",
      "Hills :  \n",
      "set(['Barkdene Hills', 'Cobble Hills'])\n",
      "Cottages :  \n",
      "set(['Wellesley Cottages'])\n"
     ]
    }
   ],
   "source": [
    "#st_types = audit_streets(source_xml_file)\n",
    "for key in st_types.keys()[0:3]:\n",
    "    print key + \" :  \" \n",
    "    pprint.pprint(st_types[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loops over all the streets and checks if there is a better\n",
    "# name for the street(get rid of typos and abbreviations)\n",
    "for st_type, ways in st_types.iteritems():\n",
    "    if st_type in mapping.keys():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print name, \"=>\", better_name\n",
    "            name = better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spadina Rd => Spadina Road<br>\n",
    "\n",
    "Kennedy Rd => Kennedy Road<br>\n",
    "\n",
    "JARVIS STREET => JARVIS Street<br>\n",
    "\n",
    "Ryerson avenue => Ryerson Avenue<br>\n",
    "\n",
    "Dovercourt => Dovercourt Road<br>\n",
    "\n",
    "Red Robinway => Red Robin Way<br>\n",
    "\n",
    "Sea Robinway => Sea Robin Way<br>\n",
    "\n",
    "Dundas street => Dundas Street<br>\n",
    "\n",
    "San Robertoway => San Roberto Way<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit / Clean PostCode\n",
    "\n",
    "There were formatting typos for postal code. Most of them were correct, only a one refered to postal code from the city of Ottawa instead of Toronto.\n",
    "\n",
    "A few mistakes were rectified by manual searching for the address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M5T 1R9, M1P 2L7\n",
      "M36 0H7\n",
      "L4K\n",
      "M5J 2G\n",
      "K4A 1W9\n"
     ]
    }
   ],
   "source": [
    "audit_postcode(source_xml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "M5T 1R9, M1P 2L7  --- M1P 2L7 is actually the wrong address<br>\n",
    "\n",
    "M36 0H7 -  should be replaced with M3C 0H7 invalid postal code<br>\n",
    "\n",
    "L4K - vaughan area code excluded<br>\n",
    "\n",
    "M5J 2G   to be replaced with M5J 2G8- invalid,  This tag is associated to a way which is wrong!<br>\n",
    "\n",
    "K4A 1W9 - ottawa area excluded, This tag is associated to a way which is wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit Bank\n",
    "\n",
    "From the analysis done, I see that all the Bank information has been specified correctly. The only concern here is there should be an additional tag to hold the information of the parent bank company. So, when there is TD Bank in 10 different locations with different names, we can know by the parent tag that they are all related to TD Bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "audit_banks(source_xml_file)\n",
    "#unique_banks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defaultdict(set,<br>\n",
    "\n",
    "<code>&nbsp;&nbsp;</code> {'BMO': {'BMO', 'BMO - Bank of Mointreal','BMO - Bank of Montreal', 'BMO Bank of Montreal', 'BMO Bank of Montreal/BMO Nesbitt Burns', 'BMO Financial Group', 'BMO Insurance', 'BMO Nesbitt Burns'},<br><code>&nbsp;&nbsp;</code>\n",
    "\n",
    "'BPI': {'BPI'},\n",
    "\n",
    "<br><code>&nbsp;&nbsp;</code>...\n",
    "\n",
    "<code>&nbsp;&nbsp;</code>'TD': {'TD', 'TD Bank Drive Through', 'TD Canada Trust', 'TD Commercial Banking', 'TD Waterhouse'},\n",
    "\n",
    "<br><code>&nbsp;&nbsp;</code>...\n",
    "\n",
    "<code>&nbsp;&nbsp;</code>'iTRADE': {'Scotia iTRADE'}})\n",
    "             \n",
    "\n",
    "\n",
    "In the output above I have written the code to group the banks, It would have been much easier if there was a tag that will hold the parent company name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML to JSON Conversion\n",
    "\n",
    "It is now time to convert the XML data to JSON and thereby loading it into mongodb to do further assessment.\n",
    "\n",
    "\n",
    "I have used more structured json format as below to keep data better organized.\n",
    "\n",
    "{<br>\n",
    "\n",
    "\"id\": ,<br>\n",
    "\n",
    "\"type\": ,\n",
    "\n",
    "\"pos\": , \n",
    "\n",
    "\"created\": {\n",
    "       \"uid\": ,\n",
    "       \"changeset\":,\n",
    "       \"version\": , \n",
    "       \"user\": ,\n",
    "       \"timestamp\":\n",
    "     }\n",
    "      \n",
    "\"features\" : { # all tag related information }\n",
    "      \n",
    "\"node_refs\" : []\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to MongoDB\n",
    "\n",
    "The data that is obtained from the previous step can be loaded into mongo via pymongo. This can also be acheived by using mongoimport to import the data from JSON file to mongodb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default connection to localhost\n",
    "client = MongoClient()\n",
    "# switch to project DB\n",
    "db=client.project\n",
    "\n",
    "# drop any data if already present in toronto collection\n",
    "db.toronto.drop()\n",
    "# insert all data \n",
    "db.toronto.insert_many(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview\n",
    "\n",
    "#### File size (in bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336400599L"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.getsize(source_xml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364747302L"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.getsize(source_xml_file+\".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OSM file size: 320 MB <br>\n",
    "\n",
    "JSON file size: 356 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####  Number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1511572\n"
     ]
    }
   ],
   "source": [
    "nb_doc=db.toronto.find().count()\n",
    "print(nb_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290711\n"
     ]
    }
   ],
   "source": [
    "nb_nodes=db.toronto.find({\"type\":\"node\"}).count()\n",
    "print(nb_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220861\n"
     ]
    }
   ],
   "source": [
    "nb_way=db.toronto.find({\"type\":\"way\"}).count()\n",
    "print(nb_way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number  of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1154\n"
     ]
    }
   ],
   "source": [
    "nb_unique_users=len(db.toronto.distinct(\"created.user\"))\n",
    "print(nb_unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 1 contributing User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top contributor is andrewpmk with - 1187521 contributions.\n"
     ]
    }
   ],
   "source": [
    "cursor=db.toronto.aggregate(\n",
    "    [\n",
    "        {\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}}, \n",
    "        {\"$sort\":{\"count\":-1}}, {\"$limit\":1}\n",
    "    ]\n",
    ")\n",
    "for res in cursor:\n",
    "    user1=res[\"_id\"]\n",
    "    user1_count=res[\"count\"]\n",
    "\n",
    "print \"Top contributor is \"+ user1 + \" with - \" + str(user1_count) + \" contributions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of users appearing only once (having 1 post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 288 users who only contributed once.\n"
     ]
    }
   ],
   "source": [
    "user_1post=db.toronto.aggregate(\n",
    "    [\n",
    "        {\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\":{\"count\":-1}},\n",
    "        {\"$match\":{\"count\":1}},\n",
    "        {\"$group\":{\"_id\":\"null\",\"total\":{\"$sum\":\"$count\"}}}\n",
    "    ]\n",
    ")\n",
    "\n",
    "for res in user_1post:\n",
    "    nb_user_1post=res[\"total\"]\n",
    "\n",
    "print \"There are \" + str(nb_user_1post) +  \" users who only contributed once.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of unique sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.toronto.distinct('feature.source'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': None, u'count': 1227435}\n",
      "{u'_id': u'CanVec 6.0 - NRCan', u'count': 174462}\n",
      "{u'_id': u'Bing', u'count': 35257}\n",
      "{u'_id': u'StatCan 92-500-X', u'count': 27284}\n",
      "{u'_id': u'Geobase_Import_2009', u'count': 15395}\n"
     ]
    }
   ],
   "source": [
    "user_1post=db.toronto.aggregate(\n",
    "    [\n",
    "        {\"$group\":{\"_id\":\"$feature.source\", \"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\":{\"count\":-1}},\n",
    "        {\"$limit\":5}\n",
    "    ]\n",
    ")\n",
    "\n",
    "for res in user_1post:\n",
    "    pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional data exploration using MongoDB queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'parking', u'count': 11592}\n",
      "{u'_id': u'fast_food', u'count': 1520}\n",
      "{u'_id': u'bench', u'count': 1473}\n",
      "{u'_id': u'restaurant', u'count': 1408}\n",
      "{u'_id': u'post_box', u'count': 1101}\n",
      "{u'_id': u'cafe', u'count': 838}\n",
      "{u'_id': u'waste_basket', u'count': 777}\n",
      "{u'_id': u'place_of_worship', u'count': 731}\n",
      "{u'_id': u'bank', u'count': 597}\n",
      "{u'_id': u'school', u'count': 467}\n"
     ]
    }
   ],
   "source": [
    "top_amenity=db.toronto.aggregate(\n",
    "    [\n",
    "        {\"$match\":{\"feature.amenity\":{\"$exists\":1}}},\n",
    "        {\"$group\":{\"_id\":\"$feature.amenity\",\"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\":{\"count\":-1}},\n",
    "        {\"$limit\":10}\n",
    "    ]\n",
    ")\n",
    "\n",
    "for res in top_amenity:\n",
    "    pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 fast-food chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'Subway', u'count': 200}\n",
      "{u'_id': u'Pizza Pizza', u'count': 99}\n",
      "{u'_id': u\"McDonald's\", u'count': 68}\n",
      "{u'_id': u'Mr. Sub', u'count': 36}\n",
      "{u'_id': u'Pizza Nova', u'count': 34}\n"
     ]
    }
   ],
   "source": [
    "top_fastfood=db.toronto.aggregate(\n",
    "    [\n",
    "        {\"$match\":{\"feature.amenity\":\"fast_food\"}},\n",
    "        {\"$group\":{\"_id\":\"$feature.name\",\"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\":{\"count\":-1}},\n",
    "        {\"$limit\":5}\n",
    "    ]\n",
    ")\n",
    "\n",
    "for res in top_fastfood:\n",
    "    pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of top 5 cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'coffee_shop', u'count': 506}\n",
      "{u'_id': u'pizza', u'count': 267}\n",
      "{u'_id': u'sandwich', u'count': 235}\n",
      "{u'_id': u'burger', u'count': 176}\n",
      "{u'_id': u'chinese', u'count': 82}\n"
     ]
    }
   ],
   "source": [
    "cuisine = db.toronto.aggregate(\n",
    "    [\n",
    "        {\"$match\":{\"feature.cuisine\":{\"$exists\":1}}},\n",
    "        {\"$group\":{\"_id\":\"$feature.cuisine\", \"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\":{\"count\":-1}},\n",
    "        {\"$limit\":5}\n",
    "    ]\n",
    ")\n",
    "for res in cuisine:\n",
    "    pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuisine tag has not relevant information in it. The only relevant cuisine from the above results is chinese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biggest Bank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'TD Canada Trust', u'count': 133}\n",
      "{u'_id': u'Scotiabank', u'count': 92}\n",
      "{u'_id': u'CIBC', u'count': 64}\n"
     ]
    }
   ],
   "source": [
    "temp=db.toronto.aggregate(\n",
    "    [\n",
    "        {\"$match\":{\"feature.amenity\":\"bank\"}},\n",
    "        {\"$group\":{\"_id\":\"$feature.name\",\"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\":{\"count\":-1}},\n",
    "        {\"$limit\":3}\n",
    "    ]\n",
    ")\n",
    "\n",
    "for res in temp:\n",
    "    pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Royal Street Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 140 of them.\n",
      "Here are a few - \n",
      "1) Queen Street West\n",
      "2) King Street West\n",
      "3) Queen's Quay West\n",
      "4) King Street East\n",
      "5) Prince Arthur Avenue\n"
     ]
    }
   ],
   "source": [
    "royal_street_names = db.toronto.distinct(\n",
    "    \"feature.address.street\", {\n",
    "        \"feature.address.street\": {\n",
    "            \"$regex\" : \"King|Queen|Prince|Princess|Royal\"\n",
    "        } \n",
    "    } \n",
    ")\n",
    "\n",
    "print \"There are \" + str(len(royal_street_names)) + \" of them.\"\n",
    "print \"Here are a few - \" \n",
    "for i, street in zip(range(1,6), royal_street_names[0:5]):\n",
    "    print str(i) + \") \" + street"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Ideas\n",
    "\n",
    "#### Contributor statistics\n",
    "\n",
    "Let's find out how much percentage of contributions made by top 5 contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_5_users = db.toronto.aggregate(\n",
    "    [\n",
    "        {\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\":{\"count\":-1}}, {\"$limit\":5}\n",
    "    ]\n",
    ")\n",
    "\n",
    "all_users_contribution = db.toronto.aggregate(\n",
    "    [\n",
    "        {\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\":{\"count\":-1}}\n",
    "    ]\n",
    ")\n",
    "total_contributions_made = 0\n",
    "for user_con in all_users_contribution:\n",
    "    total_contributions_made += user_con[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User andrewpmk has 79.0% contribution\n",
      "User Kevo has 5.0% contribution\n",
      "User Mojgan Jadidi has 2.0% contribution\n",
      "User andrewpmk_imports has 2.0% contribution\n",
      "User Bootprint has 1.0% contribution\n",
      "\n",
      "Top 5 users contribution is  89.0%\n"
     ]
    }
   ],
   "source": [
    "contributions_made_by_top_5 = 0\n",
    "for res in top_5_users:\n",
    "    contributions_made_by_top_5 += res[\"count\"]\n",
    "    print \"User \" + res[\"_id\"] + \" has \" + str(round(float(res[\"count\"] * 100)/total_contributions_made)) + \"% contribution\"\n",
    "    \n",
    "print \"\\nTop 5 users contribution is  \" + str(round(float(contributions_made_by_top_5 * 100)/total_contributions_made)) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map about the city of Toronto is relatively clean so I could retrieve some interesting content. But still the data is not entirely clean. The data contains some mistakes or different references for the same feature (like abbreviation or slightly different names). So I had to clean the data programmatically for the street and the postal code. But they are not the only features, that require cleaning. There are many tags for the key types that are ambiguous, the values entered are not appropriate as well. For example when looking at the cuisine, most the data there was entered incorrectly. \n",
    "\n",
    "\n",
    "__Ideas to improve data quality of OSM:__ <br>\n",
    "\n",
    "When we audit the data, it was very clear that although there are minor error caused by human input, the data is fairly well-cleaned. Considering there're hundreds of contributors for this map, there is a great numbers of human errors in this project. OSM gives a lot of flexibility for the user to enter new information. The data that is entered should agree with strong guidelines. I'd recommend a srtuctured input form so everyone can input the same data format. Evolving this form to allow additional information should be keenly monitored.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
